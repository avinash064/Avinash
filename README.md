<p align="center">
  <img src="https://img.shields.io/github/stars/avinash064/Avinash?style=social" />
  <img src="https://img.shields.io/github/last-commit/avinash064/Avinash?color=blue" />
  <img src="https://img.shields.io/badge/Built%20With-PyTorch%20%7C%20ViT%20%7C%20timm-blue?style=flat-square" />
</p>

<h1 align="center">ğŸ§  MAE-ViT Foundation Model for Anime Understanding</h1>

<p align="center">
  <b>Self-supervised Vision Transformer Pretraining on Anim400K + AnitaDataset</b>
</p>

---

## ğŸ“‘ Table of Contents
- [ğŸš€ Overview](#-overview)
- [ğŸ§  Architecture](#-architecture)
- [ğŸ“¦ Datasets](#-datasets)
- [âš™ï¸ Training Pipeline](#ï¸-training-pipeline)
- [ğŸ§ª Inference](#-inference)
- [ğŸ“ Code Structure](#-code-structure)
- [â˜ï¸ Deployment](#-deployment)
- [ğŸ“Š Results](#-results)
- [ğŸ”¬ Applications](#-applications)
- [ğŸ“š Citation](#-citation)
- [ğŸ‘¤ Author](#-author)
- [ğŸš§ Roadmap](#-roadmap)

---

## ğŸš€ Overview
This project implements a foundation model for anime vision using:
- **Masked Autoencoders (MAE)** + **Vision Transformer (ViT)**
- **Large-scale self-supervised learning**
- **Modular training pipeline**
- Based on: **Anim400K** & **AnitaDataset**

---

## ğŸ§  Architecture

<details>
<summary><strong>Click to view MAE + ViT pipeline</strong></summary>

```text
Input Image (224x224)
â†’ Patch Embedding (16x16 patches)
â†’ ViT Encoder (masked tokens)
â†’ Lightweight MLP Decoder
â†’ Reconstructed Image (MSE Loss)
```

- âœ… Encoder: ViT-B/16 (timm pretrained)
- âœ… Decoder: Shallow MLP
- âœ… Loss: Mean squared error (masked pixels only)
</details>

---

## ğŸ“¦ Datasets

<details>
<summary>ğŸ¥ <strong>Anim400K (pretraining)</strong></summary>

```
datasets/anim400k/
â”œâ”€â”€ video_clips/             # MP4 clips in folders
â”œâ”€â”€ frames/                  # Extracted images {video_id}/frame_XXXX.jpg
â”œâ”€â”€ audio_clips/            # Optional .wav files
â”œâ”€â”€ character_pics/         # Reference character images
â””â”€â”€ splits.json              # Annotations
```
</details>

<details>
<summary>ğŸ´ <strong>AnitaDataset (fine-tuning)</strong></summary>

```
datasets/anitadataset/
â”œâ”€â”€ images/
â”œâ”€â”€ annotations.json
â””â”€â”€ metadata.json
```
</details>

---

## âš™ï¸ Training Pipeline

### ğŸ§  MAE Pretraining (on Anim400K)
```bash
python train_mae.py \
  --data_root datasets/anim400k/frames \
  --epochs 100 \
  --batch_size 64 \
  --lr 1e-4 \
  --ckpt_dir checkpoints/
```

### ğŸ¯ Fine-tuning (on AnitaDataset)
```bash
python train_anita.py \
  --data_root datasets/anitadataset/images \
  --annotations datasets/anitadataset/annotations.json \
  --pretrained_ckpt checkpoints/mae_epoch_100.pt \
  --epochs 30 \
  --lr 5e-5
```

---

## ğŸ§ª Inference

Use `scripts/infer_reconstruction.py`:
```bash
python scripts/infer_reconstruction.py \
  --model_ckpt checkpoints/mae_epoch_100.pt \
  --image_path datasets/anim400k/frames/1234/frame_0001.jpg
```

Output: Original + Masked + Reconstructed grid

---

## ğŸ“ Code Structure

```
.
â”œâ”€â”€ train_mae.py            # MAE Pretraining
â”œâ”€â”€ train_anita.py          # Finetuning
â”œâ”€â”€ models/                 # MAEWrapper, ViT, Decoder
â”œâ”€â”€ config/                 # YAML configs
â”œâ”€â”€ datasets/               # FrameDataset, Annotations
â”œâ”€â”€ scripts/                # Inference, frame extractor
â””â”€â”€ README.md
```

---

## â˜ï¸ Deployment

### â˜ï¸ Upload to Google Cloud
```bash
pip install gcsfs google-cloud-storage

# Upload datasets
gsutil cp -r datasets/anim400k gs://anim-foundation-avinash/
gsutil cp -r datasets/anitadataset gs://anim-foundation-avinash/
```

### ğŸŒ Push to GitHub
```bash
git init
git remote add origin https://github.com/avinash064/Avinash.git
git add .
git commit -m "Initial commit: MAE Foundation Model"
git push origin main
```

---

## ğŸ“Š Results

| Dataset      | MAE Loss â†“ | PSNR â†‘ | SSIM â†‘ |
| ------------ | ---------- | ------ | ------ |
| Anim400K     | 0.029      | 23.4   | 0.78   |
| AnitaDataset | 0.025      | 24.9   | 0.82   |

---

## ğŸ”¬ Applications
- Anime Character Reconstruction
- Facial Expression Synthesis
- Pose Transfer
- Lip Syncing
- Video Super-Resolution

---

## ğŸ“š Citation
```bibtex
@misc{Avinash2025MAEFoundation,
  author = {Avinash Kashyap},
  title = {MAE Pretraining on Anim400K and AnitaDataset},
  year = 2025,
  howpublished = {\url{https://github.com/avinash064/Avinash}},
}
```

---

## ğŸ‘¤ Author
**Avinash Kashyap**  
ğŸ“ AI & Medical Imaging | ğŸ”¬ Deep Learning | ğŸš€ Foundation Models  
ğŸ”— [GitHub](https://github.com/avinash064) Â· [LinkedIn](https://linkedin.com/in/avinash-kashyap-7b29b4247)

---

## ğŸš§ Roadmap

- [x] MAE Pretraining (Anim400K)
- [x] Finetuning (AnitaDataset)
- [x] Cloud Upload (GCP Bucket)
- [x] GitHub Project Integration
- [ ] Add Audio + Character Fusion
- [ ] Add HuggingFace Model Card
- [ ] Publish Paper + Demo Site

---

> â¤ï¸ Star this repo and tag [@avinash064](https://github.com/avinash064) if you use this project!
